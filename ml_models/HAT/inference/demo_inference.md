# Inference SuperResolution Demo

## Настройка Docker образов

Инференс модели нейронной сети для Super Resolution происходит при помощи Nvidia Triton Inference Server и скриптов на Python. Сборка исходников осуществляется при помощи создания Docker образов.

Первоначально необходимо локально загрузить исходные файлы при помощи команды:

`git clone https://gitlab.compvisionsys.com/geo_ai/geo_ai_superresolution.git`

Далее необходимо перейти в каталог `/geo_ai_superresolution/HAT/` и собрать образ командой:

`docker build -t hat_triton_inference -f Dockerfile .`

После этого необходимо загрузить образ nvidia triton (~12Gb) командой:

`docker pull nvcr.io/nvidia/tritonserver:22.08-py3`


Также необходимо создать общий network для связи запускаемых контейнеров:

`docker network create`





<!-- Также запускается образ самого инференса:

`sudo docker run --network triton_network --rm -it -v ./inputs:/superresolution/inputs -v ./outputs:/superresolution/outputs hat_triton_inference bash` -->

На этом создание и загрузка необходимых образов закончена

## Описание функций для инференса

За инференс отвечает файл `triton_inference.py`, который может быть использован как через его отдельные функции, так и вызван напрямую. Описание функций:

### *img_preproc*

Принимает на вход исходное изображение в виде матрицы (h x w x 3) формата BGR. На выходе возвращает тензор, преобразованный для входа в модель нейронной сети (1 x 3 x h x w) формата RGB.

### *img_postproc*

Принимает на вход выходной тензор модели нейронной сети (1 x 3 x H x W) формата RGB. На выходе возвращает стандартную матрицу изображения формата BGR, готовую для сохранения или дальнейшего использования средствами *OpenCV*.

### *inference_triton*

Проивзодит непосредственно инференс через выбранную и развёрнутую на Triton сервере модели нейронной сети. На вход принимает тензор преобразованного исходного изображения (1 x 3 x h x w), имя используемой модели и Triton клиент, получаемый при помощи функции:

```python
triton_client = httpclient.InferenceServerClient(url=triton_server_url)
```

В качестве *triton_server_url* обычно используются 172.18.0.2:8000 при запуске через docker, либо localhost:8000 при запуске без docker. Но также может быть необходима дополнительная проверка (см. ниже).

На выходе выдаёт тензор (1 x 3 x H x W), где H и W - увеличенные высота и ширина для исходного изображения. Далее тензор может быть преобразован в вид стандартного изображения при помощи функции *img_postproc*.

### *inference_pipeline*

Упрощённая функция для получения готового увеличенного изображения из исходного. На вход принимает исходное изображение в стандартном формате (h x w x 3), имя модели и URL Triton сервера. 
На выходе выдаёт увеличенное изображение также в стандартном формате (H x W x 3).

## Примеры использования

После сборки образов Docker для Triton и нашего репозитория, необходимо произвести от них запуск контейнеров. 

Сперва запускается контейнер для Triton Inference Server:

`docker run --gpus=all --network triton_network --name triton_server --rm -p8000:8000 -p8001:8001 -p8002:8002 -v </path/to/models/>:/models nvcr.io/nvidia/tritonserver:22.08-py3 tritonserver --model-repository=/models`

Успешно запущенное окно терминала может быть оставлено. Дальнейшие действия следует проводить в другом окне. 

Для проверки http адреса запущенного контейнера необходимо использовать команду:

`docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' triton_-_server`

Полученный результат необходимо исопльзовать как первую часть url адреса Triton сервера при запуске скрипта triton_inference.py.


Далее запускается образ самого инференса:

`docker run --network triton_network --rm -it -v ./inputs:/superresolution/inputs -v ./outputs:/superresolution/outputs hat_triton_inference bash`

В результате будет запущена bash оболочка контейнера, через которую можно осуществлять связь с инференс файлом через команду:

`python triton_inference.py --weights Aerial_HAT-L_SRx2_finetune_5000 --img_dst ./inputs/3767.png --img_save ./outputs/3767.png --url 172.18.0.2:8000`

Название модели и путь ко входному и выходному изображениям может использоваться свой. Адрес сервера в большинстве случаев стандартный, но может отличаться (необходима предварительная проверка через `docker inspect`).

## Примеры изображений:

*Исходное изображение 320х320:*

<p align="center">
  <img src="../images/3768_320.png" />
</p>

___

_Обработанное изображение 1280х1280:_

<p align="center">
  <img src="../images/3768_320x4.png" />
</p>


<!-- #TODO: рассказать про все функции в скрипте для инфа, входные выходные данные, показать картинки, нарисовать схемки пайплайна и сетки, рассказать про струткутру каталогов для triton -->



